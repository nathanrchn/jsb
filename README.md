[![ArXiv](https://img.shields.io/badge/arXiv-2501.10868-b31b1b)](https://arxiv.org/abs/2501.10868)
[![Hugging Face](https://img.shields.io/badge/Dataset-Hugging%20Face-orange)](https://huggingface.co/datasets/epfl-dlab/JSONSchemaBench)

JSONSchemaBench is a library for benchmarking language models on their ability to generate valid JSON according to JSON Schema specifications. This tool allows researchers and developers to evaluate different language models on structured generation tasks.

## Overview

JSONSchemaBench provides:

- A standardized benchmark for evaluating language models on JSON Schema generation
- Support for multiple LLM engines (OpenAI, Guidance, etc.)
- Comprehensive evaluation metrics
- Easy-to-use API for running benchmarks

## Getting Started

To get started with JSONSchemaBench, see the [Quick Start Guide](docs/quickstart.md).

## Documentation Sections

- [Installation](docs/installation.md)
- [Core Concepts](docs/core_concepts.md)
- [API Reference](docs/api_reference.md)
- [Engines](docs/engines.md)
- [Evaluation Metrics](docs/evaluation_metrics.md)
- [Examples](docs/examples.md)

## GitHub Repository

The source code is available on GitHub: [jsonschemabench](https://github.com/guidance-ai/jsonschemabench) 
